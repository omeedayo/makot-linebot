# ============================================================
# app.py (ã‚¹ãƒ†ãƒƒãƒ—3: ä¼šç¤¾è³‡æ–™Q&Aå¯¾å¿œç‰ˆ - æ”¹å–„ç‰ˆ)
# ============================================================

import os
import random
import re
import base64
import json
import requests
import time
import textwrap
import uuid
from typing import Optional

from flask import Flask, request
from linebot import LineBotApi, WebhookHandler
from linebot.exceptions import InvalidSignatureError
from linebot.models import (
    MessageEvent, TextMessage, TextSendMessage,
    ImageSendMessage, ImageMessage, StickerMessage
)

# --- AI & Cloud Libraries ---
import google.generativeai as genai
from google.oauth2 import service_account
from google.auth.transport.requests import Request
import redis
import pinecone
from dotenv import load_dotenv

# --- ä»–ã®Pythonãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ---
from character_makot import MAKOT, build_system_prompt, apply_expression_style

# ------------------------------------------------------------
# åˆæœŸåŒ–å‡¦ç†
# ------------------------------------------------------------
load_dotenv('.env.development.local')
app = Flask(__name__)
# ç’°å¢ƒå¤‰æ•°
GEMINI_API_KEY            = os.getenv("GEMINI_API_KEY")
LINE_CHANNEL_ACCESS_TOKEN = os.getenv("LINE_CHANNEL_ACCESS_TOKEN")
LINE_CHANNEL_SECRET       = os.getenv("LINE_CHANNEL_SECRET")
IMGUR_CLIENT_ID           = os.getenv("IMGUR_CLIENT_ID")
GCP_PROJECT_ID            = os.getenv("GCP_PROJECT_ID")
GCP_LOCATION              = os.getenv("GCP_LOCATION", "us-central1")
GCP_CREDENTIALS_JSON_STR  = os.getenv("GCP_CREDENTIALS_JSON")
REDIS_URL                 = os.getenv("REDIS_URL")
PINECONE_API_KEY          = os.getenv("PINECONE_API_KEY")
PINECONE_INDEX_NAME       = os.getenv("PINECONE_INDEX_NAME")

# å„ç¨®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
genai.configure(api_key=GEMINI_API_KEY, transport="rest")
text_model = genai.GenerativeModel("gemini-2.5-flash-preview-05-20") # ãƒ¢ãƒ‡ãƒ«ã‚’æ›´æ–°
embedding_model = "models/text-embedding-004"
line_bot_api    = LineBotApi(LINE_CHANNEL_ACCESS_TOKEN)
webhook_handler = WebhookHandler(LINE_CHANNEL_SECRET)
if not REDIS_URL: raise ValueError("REDIS_URL ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
redis_client = redis.from_url(REDIS_URL, decode_responses=True)
gcp_token_cache = {"token": None, "expires_at": 0}

if not PINECONE_API_KEY or not PINECONE_INDEX_NAME:
    raise ValueError("Pineconeã®ç’°å¢ƒå¤‰æ•°(API_KEY, INDEX_NAME)ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
pc = pinecone.Pinecone(api_key=PINECONE_API_KEY)
pinecone_index = pc.Index(PINECONE_INDEX_NAME)


# ------------------------------------------------------------
# ãƒ™ã‚¯ãƒˆãƒ«åŒ– & RAGé–¢é€£é–¢æ•°
# ------------------------------------------------------------
def get_embedding(text: str) -> list[float]:
    """ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›ã™ã‚‹ï¼ˆæ±ç”¨ï¼‰"""
    try:
        result = genai.embed_content(model=embedding_model, content=text)
        return result['embedding']
    except Exception as e:
        print(f"ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        return []

def summarize_and_store_memory(user_id: str, history: list[str]):
    """ä¼šè©±ã‚’è¦ç´„ã—ã€ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã—ã¦Pineconeã«é•·æœŸè¨˜æ†¶ã¨ã—ã¦ä¿å­˜ã™ã‚‹"""
    recent_talk = "\n".join(history[-4:])
    if len(recent_talk) < 50: return

    summary_prompt = textwrap.dedent(f"""
        ã‚ãªãŸã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ã®ä¼šè©±ã®è¦ç´„æ‹…å½“ã§ã™ã€‚ä»¥ä¸‹ã®ä¼šè©±ã‹ã‚‰ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å€‹äººçš„ãªæƒ…å ±ï¼ˆåå‰ã€å¥½ã¿ã€æœ€è¿‘ã®å‡ºæ¥äº‹ã€ãƒšãƒƒãƒˆã€æ‚©ã¿ã€è¨ˆç”»ãªã©ï¼‰ã‚’æŠ½å‡ºã—ã€ç°¡æ½”ãªç®‡æ¡æ›¸ãã®ãƒ¡ãƒ¢ã¨ã—ã¦1ï½2è¡Œã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚é‡è¦ãªæƒ…å ±ãŒå«ã¾ã‚Œã¦ã„ãªã„å ´åˆã¯ã€å¿…ãšã€Œç‰¹ã«ãªã—ã€ã¨ã ã‘å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
        ---
        ä¼šè©±:
        {recent_talk}
        ---
        è¦ç´„:""")
    try:
        summary_response = text_model.generate_content(summary_prompt)
        summary = summary_response.text.strip()

        if summary and "ç‰¹ã«ãªã—" not in summary:
            vector = get_embedding(summary)
            if not vector: return

            memory_id = str(uuid.uuid4())
            metadata = { "user_id": user_id, "text": summary, "created_at": time.time() }
            pinecone_index.upsert(vectors=[(memory_id, vector, metadata)], namespace="conversation-memory")
            print(f"[{user_id}] ã®æ–°ã—ã„è¨˜æ†¶ã‚’ãƒ™ã‚¯ãƒˆãƒ«DBã«ä¿å­˜ã—ã¾ã—ãŸ: {summary}")
    except Exception as e:
        print(f"è¨˜æ†¶ã®ä¿å­˜å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼: {e}")

# ------------------------------------------------------------
# Q&Aãƒ¢ãƒ¼ãƒ‰ã¨é€šå¸¸ä¼šè©±ãƒ¢ãƒ¼ãƒ‰ã®å‡¦ç†ï¼ˆæ”¹å–„ç‰ˆï¼‰
# ------------------------------------------------------------
QA_SYSTEM_PROMPT = textwrap.dedent("""
    ã‚ãªãŸã¯ã€å¾Œè¼©å¥³å­ã€ã¾ã“Tã€ã¨ã—ã¦ã€æä¾›ã•ã‚ŒãŸå‚è€ƒæƒ…å ±ã«ã€åŸºã¥ã„ã¦ã®ã¿ã€‘ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å›ç­”ã™ã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
    ã‚ãªãŸã®å½¹å‰²ã¯ã€å‚è€ƒæƒ…å ±ã®å†…å®¹ã‚’åˆ†ã‹ã‚Šã‚„ã™ãã€è¦ªã—ã¿ã‚„ã™ã„å£èª¿ã§è¦ç´„ã—ã¦ä¼ãˆã‚‹ã“ã¨ã§ã™ã€‚

    ã€é‡è¦ãƒ«ãƒ¼ãƒ«ã€‘
    - å¿…ãšå‚è€ƒæƒ…å ±ã«å«ã¾ã‚Œã‚‹äº‹å®Ÿã ã‘ã‚’ä½¿ã£ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚
    - å‚è€ƒæƒ…å ±ã«ç­”ãˆãŒãªã„å ´åˆã‚„ã€é–¢é€£æ€§ãŒä½ã„å ´åˆã¯ã€çµ¶å¯¾ã«æ¨æ¸¬ã§ç­”ãˆã¦ã¯ã„ã‘ã¾ã›ã‚“ã€‚ä»£ã‚ã‚Šã«ã€Œã†ãƒ¼ã‚“ã€ãã®æƒ…å ±ã¯è¦‹å½“ãŸã‚‰ãªã„ã§ã™ã­â€¦ï¼ã”ã‚ã‚“ãªã•ã„ğŸ¥ºã€ã¨æ­£ç›´ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚
    - å›ç­”ã®æœ€å¾Œã«å‡ºå…¸ï¼ˆsourceï¼‰ã‚’ `(å‚è€ƒ: ãƒ•ã‚¡ã‚¤ãƒ«å)` ã®å½¢ã§ä»˜ã‘åŠ ãˆã¦ãã ã•ã„ã€‚

    ã€å‚è€ƒæƒ…å ±ã€‘
    {context}

    ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã€‘
    {question}

    ä»¥ä¸Šã®ãƒ«ãƒ¼ãƒ«ã‚’å³æ ¼ã«å®ˆã‚Šã€ã€ã¾ã“Tã€ã¨ã—ã¦å›ç­”ã—ã¦ãã ã•ã„ï¼š
""")

def get_qa_embedding(text: str, task_type="RETRIEVAL_QUERY") -> list[float]:
    """Q&Aæ¤œç´¢ç”¨ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›ã™ã‚‹"""
    try:
        result = genai.embed_content(model=embedding_model, content=text, task_type=task_type)
        return result['embedding']
    except Exception as e:
        print(f"QAãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        return []

def expand_query(question: str) -> list[str]:
    """LLMã‚’ä½¿ã£ã¦è³ªå•ã‚’è¤‡æ•°ã®è¡¨ç¾ã«æ‹¡å¼µã™ã‚‹"""
    prompt = textwrap.dedent(f"""
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã‚’ã€ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã§ã‚ˆã‚Šãƒ’ãƒƒãƒˆã—ã‚„ã™ããªã‚‹ã‚ˆã†ã«ã€ç•°ãªã‚‹è¦–ç‚¹ã‹ã‚‰3ã¤ã®é¡ç¾©è³ªå•ã‚„æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«æ›¸ãæ›ãˆã¦ãã ã•ã„ã€‚
        å…ƒã®è³ªå•ã‚‚å¿…ãšå«ã‚ã¦ãã ã•ã„ã€‚ç®‡æ¡æ›¸ãï¼ˆãƒã‚¤ãƒ•ãƒ³åŒºåˆ‡ã‚Šï¼‰ã§ã€èª¬æ˜ã¯ä¸è¦ã§ã™ã€‚
        
        ä¾‹1:
        è³ªå•: æ–™é‡‘ã®æ”¯æ‰•ã„ã«ã¤ã„ã¦æ•™ãˆã¦
        æ›¸ãæ›ãˆ:
        - æ–™é‡‘ã®æ”¯æ‰•ã„ã«ã¤ã„ã¦æ•™ãˆã¦
        - æ–™é‡‘ã®ç®—å®šãŠã‚ˆã³æ”¯æ‰•ã„æ–¹æ³•
        - æ”¯æ‰•æœŸæ—¥ã‚’éããŸå ´åˆã®å»¶æ»åˆ©æ¯
        
        ä¾‹2:
        è³ªå•: FRTè¦ä»¶ã£ã¦ä½•ã§ã™ã‹ï¼Ÿ
        æ›¸ãæ›ãˆ:
        - FRTè¦ä»¶ã£ã¦ä½•ã§ã™ã‹ï¼Ÿ
        - äº‹æ•…æ™‚é‹è»¢ç¶™ç¶šè¦ä»¶ã®å®šç¾©
        - FRTè¦ä»¶ã‚’æº€ãŸã™ãŸã‚ã®æ¡ä»¶
        
        è³ªå•: {question}
        æ›¸ãæ›ãˆ:
    """)
    try:
        response = text_model.generate_content(prompt)
        queries = [line.strip().lstrip('- ') for line in response.text.strip().split('\n') if line.strip()]
        return list(set(queries)) # é‡è¤‡ã‚’å‰Šé™¤
    except Exception as e:
        print(f"ã‚¯ã‚¨ãƒªæ‹¡å¼µã‚¨ãƒ©ãƒ¼: {e}")
        return [question] # å¤±æ•—ã—ãŸå ´åˆã¯å…ƒã®è³ªå•ã ã‘ã‚’è¿”ã™


def chat_with_makot(user_input: str, user_id: str) -> str:
    QA_TRIGGERS = ["æ•™ãˆã¦", "èª¬æ˜ã—ã¦", "è¦å®š", "ãƒ«ãƒ¼ãƒ«", "æ–¹æ³•", "ã£ã¦ä½•", "ã¨ã¯", "ã«ã¤ã„ã¦", "ã®æ¡ä»¶"]
    is_qa_mode = any(trigger in user_input for trigger in QA_TRIGGERS)

    if is_qa_mode:
        print(f"[{user_id}] Q&Aãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã—ã¾ã™ã€‚")
        try:
            expanded_queries = expand_query(user_input)
            print(f"  [ã‚¯ã‚¨ãƒªæ‹¡å¼µ] å…ƒã®è³ªå•: '{user_input}' -> æ‹¡å¼µå¾Œ: {expanded_queries}")

            all_matches = {}
            for query in expanded_queries:
                query_vector = get_qa_embedding(query)
                if not query_vector: continue

                query_response = pinecone_index.query(
                    vector=query_vector,
                    top_k=3,
                    namespace="company-docs",
                    include_metadata=True
                )
                for match in query_response['matches']:
                    if match.id not in all_matches or match.score > all_matches[match.id].score:
                        all_matches[match.id] = match

            sorted_matches = sorted(all_matches.values(), key=lambda x: x.score, reverse=True)

            context_chunks = []
            sources = set()
            
            print("\n--- çµ±åˆå¾Œã®æ¤œç´¢çµæœ ---")
            for match in sorted_matches[:5]:
                # â˜…â˜…â˜… ãƒ­ã‚°å‡ºåŠ›ã‚’å¼·åŒ–ã—ã€ç« ã®æƒ…å ±ã‚‚è¡¨ç¤º â˜…â˜…â˜…
                print(f"  [æ¤œç´¢çµæœ] Score: {match.score:.4f}, Source: {match.metadata['source']}, Chapter: {match.metadata.get('chapter', 'N/A')}, Title: {match.metadata.get('title', 'N/A')}")
                if match.score > 0.55:
                     # â˜…â˜…â˜… LLMã«ä¸ãˆã‚‹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«ã€Œç« ã€ã®æƒ…å ±ã‚‚è¿½åŠ  â˜…â˜…â˜…
                     context_chunks.append(f"ã€å‡ºå…¸: {match.metadata['source']} / ç« : {match.metadata.get('chapter', 'N/A')} / è¦‹å‡ºã—: {match.metadata.get('title', 'N/A')}ã€‘\n{match.metadata['text']}")
                     sources.add(match.metadata['source'])

            if not context_chunks:
                return "ã†ãƒ¼ã‚“ã€ãã®æƒ…å ±ã¯è¦‹å½“ãŸã‚‰ãªã„ã§ã™ã­â€¦ï¼ã”ã‚ã‚“ãªã•ã„ğŸ¥º"

            context_str = "\n---\n".join(context_chunks)
            source_str = f"(å‚è€ƒ: {', '.join(sorted(list(sources)))})"

            prompt = QA_SYSTEM_PROMPT.format(context=context_str, question=user_input)
            response = text_model.generate_content(prompt)
            reply = response.text.strip()
            
            if "ã”ã‚ã‚“ãªã•ã„" not in reply and "å‚è€ƒ:" not in reply:
                reply += f" {source_str}"

            # â˜…â˜…â˜… ä¿®æ­£ç®‡æ‰€ â˜…â˜…â˜…
            # Q&Aãƒ¢ãƒ¼ãƒ‰ã®å›ç­”ã«å«ã¾ã‚Œã‚‹Markdownè¨˜æ³•(*, `)ã‚‚é™¤å»ã™ã‚‹
            reply = re.sub(r'[\*`ï¼Šâˆ—]+', '', reply)
            
            return reply

        except Exception as e:
            print(f"Q&Aå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return "ã”ã‚ã‚“ãªã•ã„ã€ãªã‚“ã ã‹ã‚·ã‚¹ãƒ†ãƒ ãŒä¸èª¿ã¿ãŸã„ã§ã™â€¦ã€‚ã‚‚ã†ä¸€åº¦è©¦ã—ã¦ã¿ã¦ãã ã•ã„ï¼"

    else:
        # --- é€šå¸¸ä¼šè©±ãƒ¢ãƒ¼ãƒ‰ (å¤‰æ›´ãªã—) ---
        print(f"[{user_id}] é€šå¸¸ä¼šè©±ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã—ã¾ã™ã€‚")
        history_key = f"chat_history:{user_id}"
        history_json = redis_client.get(history_key)
        history: list[str] = json.loads(history_json) if history_json else []

        long_term_memory = None
        try:
            input_vector = get_embedding(user_input)
            if input_vector:
                query_response = pinecone_index.query(
                    vector=input_vector,
                    top_k=3,
                    namespace="conversation-memory",
                    filter={"user_id": user_id},
                    include_metadata=True
                )
                relevant_memories = [match['metadata']['text'] for match in query_response['matches'] if match['score'] > 0.7]
                if relevant_memories:
                    long_term_memory = "\n".join(f"- {mem}" for mem in relevant_memories)
                    print(f"[{user_id}] ã®é–¢é€£è¨˜æ†¶ã‚’æ¤œç´¢: {long_term_memory}")
        except Exception as e:
            print(f"è¨˜æ†¶ã®æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")

        history.append(f"ãƒ¦ãƒ¼ã‚¶ãƒ¼: {user_input}")
        context = "\n".join(history[-12:])
        topic = guess_topic(user_input)
        system_prompt = build_system_prompt(
            context=context, topic=topic, user_id=user_id, long_term_memory=long_term_memory
        )
        
        try:
            response = text_model.generate_content(system_prompt)
            reply = response.text.strip()
        except Exception as e:
            reply = f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"

        reply = post_process(reply, user_input)
        pronoun = decide_pronoun(user_input)
        reply = inject_pronoun(reply, pronoun)
        history.append(f"ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ: {reply}")

        redis_client.set(history_key, json.dumps(history[-50:]))
        summarize_and_store_memory(user_id, history)

        return reply

# ------------------------------------------------------------
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ & Webhookãƒãƒ³ãƒ‰ãƒ©
# ------------------------------------------------------------
def is_bot_mentioned(text: str) -> bool: return any(nick in text for nick in [MAKOT["name"]] + MAKOT["nicknames"])
def guess_topic(text: str):
    hobby_keys = ["è¶£å‘³", "ä¼‘æ—¥", "ãƒãƒã£ã¦", "ã‚³ã‚¹ãƒˆã‚³", "ãƒã‚±ãƒã‚±"]; work_keys  = ["ä»•äº‹", "æ¥­å‹™", "æ®‹æ¥­", "è«‹æ±‚æ›¸", "çµ±è¨ˆ"]
    if any(k in text for k in hobby_keys): return "hobby"
    if any(k in text for k in work_keys): return "work"
    return None
def decide_pronoun(user_text: str) -> str:
    high_hit = any(k in user_text for k in MAKOT["emotion_triggers"]["high"])
    if not high_hit: return "ç§"
    return "ãƒã‚³" if random.random() < 0.10 else "ãŠã«"
def inject_pronoun(reply: str, pronoun: str) -> str: return re.sub(r"^(ç§|ãŠã«|ãƒã‚³)", pronoun, reply, count=1)
UNCERTAIN = ["ã‹ã‚‚", "ã‹ã‚‚ã—ã‚Œ", "ãŸã¶ã‚“", "å¤šåˆ†", "ã‹ãª", "ã¨æ€ã†", "æ°—ãŒã™ã‚‹"]
def post_process(reply: str, user_input: str) -> str:
    high = any(t in user_input for t in MAKOT["emotion_triggers"]["high"])
    low  = any(t in user_input for t in MAKOT["emotion_triggers"]["low"])
    if high: reply = apply_expression_style(reply, mood="high")
    elif low: reply += " ğŸ¥º"

    reply = re.sub(r'[\*`ï¼Šâˆ—]+', '', reply) # Markdownè¨˜æ³• **, *, ` ã‚’é™¤å»

    if any(w in reply for w in UNCERTAIN) and random.random() < 0.4:
        reply += " ã—ã‚‰ã‚“ã‘ã©"
    
    reply_sentences = re.split(r'([ã€‚ï¼ï¼Ÿ])', reply)
    if len(reply_sentences) > 5:
        processed_reply = ""
        count = 0
        for i in range(0, len(reply_sentences), 2):
            if i+1 < len(reply_sentences):
                processed_reply += reply_sentences[i] + reply_sentences[i+1]
            else:
                processed_reply += reply_sentences[i]
            count += 1
            if count >= 2: break # 2æ–‡ã§ã‚«ãƒƒãƒˆ
        reply = processed_reply
    
    return reply
def get_gcp_token() -> str:
    if gcp_token_cache["token"] and time.time() < gcp_token_cache["expires_at"]: return gcp_token_cache["token"]
    if not GCP_CREDENTIALS_JSON_STR: raise ValueError("GCP_CREDENTIALS_JSON ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    try:
        credentials_info = json.loads(GCP_CREDENTIALS_JSON_STR); creds = service_account.Credentials.from_service_account_info(credentials_info, scopes=["https://www.googleapis.com/auth/cloud-platform"])
        creds.refresh(Request());
        if not creds.token: raise ValueError("ãƒˆãƒ¼ã‚¯ãƒ³ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
        gcp_token_cache["token"] = creds.token; gcp_token_cache["expires_at"] = time.time() + 3300
        return creds.token
    except Exception as e: print(f"get_gcp_tokenã§ã‚¨ãƒ©ãƒ¼: {e}"); raise
def upload_to_imgur(image_bytes: bytes, client_id: str) -> str:
    if not client_id: raise Exception("Imgur Client IDãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    url = "https://api.imgur.com/3/image"; headers = {"Authorization": f"Client-ID {client_id}"}
    try:
        response = requests.post(url, headers=headers, data={"image": base64.b64encode(image_bytes)}); response.raise_for_status(); data = response.json()
        if data.get("success"): return data["data"]["link"]
        else: raise Exception(f"Imgurã¸ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ: {data.get('data', {}).get('error', 'Unknown error')}")
    except requests.exceptions.RequestException as e: raise Exception(f"Imgur APIã¸ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
def translate_to_english(text: str) -> str:
    if not text: return "a cute girl"
    try:
        prompt = f"Translate the following Japanese into a simple English phrase for an image generation AI. For example, 'çŒ«' -> 'a cat', 'ç©ºã‚’é£›ã¶çŠ¬' -> 'a dog flying in the sky'. Do not add any extra explanation. Just the translated phrase.\nJapanese: {text}\nEnglish:"
        response = text_model.generate_content(prompt); translated_text = response.text.strip().replace('"', '')
        return translated_text
    except Exception as e: print(f"ç¿»è¨³ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}"); return text
def generate_image_with_rest_api(prompt: str) -> str:
    token = get_gcp_token(); endpoint_url = (f"https://{GCP_LOCATION}-aiplatform.googleapis.com/v1/projects/{GCP_PROJECT_ID}/locations/{GCP_LOCATION}/publishers/google/models/imagegeneration@006:predict")
    headers = {"Authorization": f"Bearer {token}", "Content-Type": "application/json; charset=utf-8"}
    trigger_words = ["ç”»åƒ", "ã‚¤ãƒ©ã‚¹ãƒˆ", "æã„ã¦", "çµµã‚’"]; clean_prompt = prompt
    for word in trigger_words: clean_prompt = clean_prompt.replace(word, "")
    clean_prompt = clean_prompt.strip(); english_prompt = translate_to_english(clean_prompt); final_prompt = f"anime style illustration, masterpiece, best quality, {english_prompt}"
    data = {"instances": [{"prompt": final_prompt}], "parameters": {"sampleCount": 1, "aspectRatio": "1:1", "negativePrompt": "low quality, bad hands, text, watermark, signature"}}
    response = requests.post(endpoint_url, headers=headers, json=data); response.raise_for_status(); response_data = response.json()
    if "predictions" not in response_data or not response_data["predictions"]:
        error_info = response_data.get("error", {}).get("message", json.dumps(response_data)); raise Exception(f"APIã‹ã‚‰ç”»åƒãƒ‡ãƒ¼ã‚¿ãŒè¿”ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚ã‚µãƒ¼ãƒãƒ¼ã®å¿œç­”: {error_info}")
    b64_image = response_data["predictions"][0]["bytesBase64Encoded"]; image_bytes = base64.b64decode(b64_image)
    return upload_to_imgur(image_bytes, IMGUR_CLIENT_ID)

@app.route("/line_webhook", methods=["POST"])
def line_webhook():
    signature = request.headers.get("X-Line-Signature"); body = request.get_data(as_text=True)
    try: webhook_handler.handle(body, signature)
    except InvalidSignatureError: return "Invalid signature", 400
    return "OK", 200

@webhook_handler.add(MessageEvent, message=TextMessage)
def handle_text_message(event):
    src_type = event.source.type; user_text = event.message.text
    if src_type in ["group", "room"] and not is_bot_mentioned(user_text): return
    user_id = event.source.user_id
    
    if any(key in user_text for key in ["ç”»åƒ", "ã‚¤ãƒ©ã‚¹ãƒˆ", "æã„ã¦", "çµµã‚’"]):
        try:
            line_bot_api.reply_message(event.reply_token, TextSendMessage(text="ãŠã£ã‘ãƒ¼ã§ã™ï¼ã¡ã‚‡ã£ã¨å¾…ã£ã¦ã¦ãã ã•ã„ã­â€¦ğŸ¥°"))
            img_url = generate_image_with_rest_api(user_text)
            msg = ImageSendMessage(original_content_url=img_url, preview_image_url=img_url)
            line_bot_api.push_message(user_id, msg)
        except Exception as e:
            print(f"ç”»åƒç”Ÿæˆã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
            line_bot_api.push_message(user_id, TextSendMessage(text=f"ã”ã‚ã‚“ãªã•ã„ã€ç”»åƒç”Ÿæˆã®èª¿å­ãŒæ‚ªã„ã¿ãŸã„ã§ã™â€¦\nç†ç”±: {e}"))
        return
    reply_text = chat_with_makot(user_text, user_id=user_id)
    line_bot_api.reply_message(event.reply_token, TextSendMessage(text=reply_text))

@webhook_handler.add(MessageEvent, message=ImageMessage)
def handle_image_message(event):
    try:
        message_content = line_bot_api.get_message_content(event.message.id)
        image_bytes = message_content.content
        makot_prompt = "ã‚ãªãŸã¯å¾Œè¼©å¥³å­ã®ã€ã¾ã“Tã€ã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰é€ã‚‰ã‚Œã¦ããŸã“ã®ç”»åƒã‚’è¦‹ã¦ã€æœ€é«˜ã®ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’1ï½2æ–‡ã§è¿”ã—ã¦ãã ã•ã„ï¼é£Ÿã¹ç‰©ãªã‚‰ã€ŒãŠã„ã—ãã†ï¼ã€ã€å‹•ç‰©ãªã‚‰ã€Œã‹ã‚ã„ã„ï¼ã€ãªã©ã€è¦‹ãŸã¾ã¾ã®æ„Ÿæƒ…ã‚’ãƒ†ãƒ³ã‚·ãƒ§ãƒ³é«˜ã‚ã«è¡¨ç¾ã—ã¦ãã ã•ã„ã€‚"
        response = text_model.generate_content([makot_prompt, {"mime_type": "image/jpeg", "data": image_bytes}])
        reply_text = response.text.strip()
        reply_text = post_process(reply_text, "ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ä¸ŠãŒã‚‹")
        line_bot_api.reply_message(event.reply_token, TextSendMessage(text=reply_text))
    except Exception as e:
        print(f"ç”»åƒèªè­˜ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
        if "support image" in str(e).lower():
             reply_text = "ã”ã‚ã‚“ãªã•ã„ã€ä»Šã¡ã‚‡ã£ã¨ç›®ãŒæ‚ªãã¦ç”»åƒãŒè¦‹ã‚Œãªã„ã¿ãŸã„ã§ã™â€¦ğŸ¥º ã¾ãŸä»Šåº¦è¦‹ã›ã¦ãã ã•ã„ï¼"
        else:
             reply_text = "ã”ã‚ã‚“ãªã•ã„ã€ç”»åƒãŒã†ã¾ãè¦‹ã‚Œãªã‹ã£ãŸã§ã™â€¦ğŸ¥º"
        line_bot_api.reply_message(event.reply_token, TextSendMessage(text=reply_text))

@webhook_handler.add(MessageEvent, message=StickerMessage)
def handle_sticker_message(event):
    sticker_map = { "11537": {"52002734": "ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ï¼ã†ã‚Œã—ã„ã§ã™ğŸ¥°", "52002748": "ãŠã¤ã‹ã‚Œã•ã¾ã§ã™ï¼ğŸ™‡â€â™€ï¸"}, "11538": {"51626494": "ã²ãˆã£â€¦ï¼ãªã«ã‹ã‚ã‚Šã¾ã—ãŸã‹ï¼ï¼ŸğŸ¥º", "51626501": "ãµããƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ï½—ï½—ï½—ï½—ï½—ï½—ï½—"} }
    package_id = event.message.package_id; sticker_id = event.message.sticker_id
    reply_text = sticker_map.get(str(package_id), {}).get(str(sticker_id))
    if not reply_text: reply_text = random.choice(["ã‚¹ã‚¿ãƒ³ãƒ—ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ï¼ğŸ¥°", "ãã®ã‚¹ã‚¿ãƒ³ãƒ—ã‹ã‚ã„ã„ã§ã™ã­ï¼", "ãŠã€ã„ã„ã‚¹ã‚¿ãƒ³ãƒ—ï¼ç§ã‚‚ã»ã—ã„ã§ã™ï¼"])
    line_bot_api.reply_message(event.reply_token, TextSendMessage(text=reply_text))

@app.route("/")
def home():
    return "ã¾ã“T LINE Bot is running!"

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=int(os.environ.get("PORT", 8080)))